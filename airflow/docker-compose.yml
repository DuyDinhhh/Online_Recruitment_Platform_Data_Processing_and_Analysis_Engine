#version: '3.0'
#
#services:
#  etl-pipeline-container:
#    image: apache/airflow:2.3.0-python3.8
#    volumes:
#      - /Users/nguyentadinhduy/Documents/SQL_THLONG/DE_Gen6_DataEngineering/Class8/airflow-etl/dags:/opt/airflow/dags
#      - /Users/nguyentadinhduy/spark-3.5.0-bin-hadoop3:/opt/spark
#      - /Users/Java/JavaVirtualMachines/jdk-20.jdk/Contents/Home:/opt/java
#    ports:
#      - 8080:8080
##    command: bash -c '(pip install findspark && airflow db init && airflow users create --username admin --password admin --firstname Duy --lastname Dinh --role Admin --email duynene2004@gmail.com);apt-get update && apt-get install -y openjdk-11-jdk && apt-get clean && rm -rf /var/lib/apt/lists/* && export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64;airflow webserver & airflow scheduler'
#    command: bash -c '(pip install findspark && airflow db init && airflow users create --username admin --password admin --firstname Duy --lastname Dinh --role Admin --email duynene2004@gmail.com ); su root -c "apt-get update && apt-get install -y openjdk-11-jdk && apt-get clean && rm -rf /var/lib/apt/lists/* && export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" &&airflow webserver & airflow scheduler'

version: '3.0'

services:
  etl-pipeline-container:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - /Users/nguyentadinhduy/Documents/SQL_THLONG/DE_Gen6_DataEngineering/Class8/airflow-etl/dags:/opt/airflow/dags
      - /Users/nguyentadinhduy/spark-3.5.0-bin-hadoop3:/opt/spark
    ports:
      - 8080:8080
#    environment:
#      - JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
    command: bash -c 'airflow db init && airflow users create --username admin --password admin --firstname Duy --lastname Dinh --role Admin --email duynene2004@gmail.com && airflow webserver & airflow scheduler'