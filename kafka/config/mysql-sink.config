 Connector name
name=custom-mysql-sink-connector

# Connector class, change if you are using a different connector
connector.class=io.confluent.connect.jdbc.JdbcSinkConnector

# Maximum number of tasks to use for this connector
tasks.max=1

# The Kafka topic to read from
topics=test1


# Database connection configuration
# connection.url=jdbc:mysql://localhost:3307/data_engineering
connection.url=jdbc:postgresql://localhost:5432/data_engineering
# connection.user=root
connection.user=postgres
connection.password=root


# connection.driver.class=mysql-connector-java-8.0.30.jar

# Specifies the action to be taken when data is received
insert.mode=insert

# Defines how the connector should deal with primary keys
# pk.mode=none

# Defines the handling of null records
# delete.enabled=false
delete.enabled=false
# If true, the connector can create tables in the database if they don't already exist
auto.create=true

# If true, add columns in the database table if they don't already exist
auto.evolve=true

# The name of the table in the database to store the data
table.name.format=events

pk.mode= record_key
pk.fields= job_id
# Specifies the format of the data in Kafka
# key.converter=org.apache.kafka.connect.storage.JsonConverter
# key.converter=org.apache.kafka.connect.json.StringConverter
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.storage.StringConverter

# Converter-specific settings
key.converter.schemas.enable=false
value.converter.schemas.enable=false
 