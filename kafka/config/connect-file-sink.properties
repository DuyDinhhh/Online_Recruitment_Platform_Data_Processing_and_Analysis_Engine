# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# # Connector name
# name=custom-mysql-sink-connector

# # Connector class, change if you are using a different connector
# connector.class=io.confluent.connect.jdbc.JdbcSinkConnector

# # Maximum number of tasks to use for this connector
# tasks.max=1

# # The Kafka topic to read from
# topics=test5


# # Database connection configuration
# connection.url=jdbc:mysql://localhost:3307/data_engineering
# # connection.url=jdbc:postgresql://localhost:5432/data_engineering
# connection.user=root
# # connection.user=postgres
# connection.password=root


# # connection.driver.class=mysql-connector-java-8.0.30.jar

# # Specifies the action to be taken when data is received
# insert.mode=insert

# # Defines how the connector should deal with primary keys
# pk.mode=none

# # Defines the handling of null records
# delete.enabled=false
# # delete.enabled=true
# # If true, the connector can create tables in the database if they don't already exist
# auto.create=true

# # If true, add columns in the database table if they don't already exist
# auto.evolve=true

# # The name of the table in the database to store the data
# table.name.format=events

# # pk.mode= record_key
# # pk.fields= job_id
# # Specifies the format of the data in Kafka
# # key.converter=org.apache.kafka.connect.storage.JsonConverter
# # key.converter=org.apache.kafka.connect.json.StringConverter
# # key.converter=org.apache.kafka.connect.storage.StringConverter
# # value.converter=org.apache.kafka.connect.storage.StringConverter

# # # errors.tolerance=all
# # value.converter=org.apache.kafka.connect.json.JsonConverter
# # value.converter.schemas.enable=false
# # key.converter=org.apache.kafka.connect.json.JsonConverter
# # # key.converter=org.apache.kafka.connect.storage.StringConverter
# # key.converter.schemas.enable=false



# ##### 

# value.converter.schemas.enable=true
# value.converter=io.confluent.connect.json.JsonSchemaConverter
# value.converter.schema.registry.url=http://localhost:8081
# key.converter=org.apache.kafka.connect.storage.StringConverter
# key.converter.schemas.enable=true







 
name=custom-mysql-sink-connector
connector.class=io.confluent.connect.jdbc.JdbcSinkConnector
tasks.max=1
topics=test5
connection.url=jdbc:mysql://localhost:3307/data_engineering
connection.user=root
connection.password=root
insert.mode=insert
pk.mode=kafka
pk.fields=job_id
delete.enabled=false
auto.create=true
auto.evolve=true
table.name.format=events
# value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=false
# key.converter=org.apache.kafka.connect.json.JsonConverter

key.converter.schemas.enable=false
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.storage.StringConverter

